{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Stacking LR&GB = TOP1 [0.10649] {House Prices}** \n",
    " - https://www.kaggle.com/itslek/blend-stack-lr-gb-0-10649-house-prices-v57/data?scriptVersionId=11189608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  #\n",
    "from datetime import datetime\n",
    "\n",
    "from data_processing import data_process\n",
    "\n",
    "from scipy.stats import skew  # for some statistics\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ML 2019-11-18 14:35:47.724269\n",
      "TEST score on CV\n",
      "Kernel Ridge score: 0.1061 (0.0107)\n",
      " 2019-11-18 14:36:08.348185\n",
      "Lasso score: 0.1057 (0.0108)\n",
      " 2019-11-18 14:36:21.109104\n",
      "ElasticNet score: 0.1056 (0.0107)\n",
      " 2019-11-18 14:37:12.067938\n",
      "SVR score: 0.1798 (0.0266)\n",
      " 2019-11-18 14:37:21.006310\n",
      "GradientBoosting score: 0.1083 (0.0083)\n",
      " 2019-11-18 14:39:03.800147\n"
     ]
    }
   ],
   "source": [
    "print('START ML', datetime.now(), )\n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "df_processed = data_process()\n",
    "X = df_processed.drop([\"SalePrice\"], axis=1)\n",
    "y = df_processed.loc[:,'SalePrice']\n",
    "X_test = data_process(test = True)\n",
    "\n",
    "\n",
    "\n",
    "# rmsle\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "# build our model scoring function\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
    "                                    scoring=\"neg_mean_squared_error\",\n",
    "                                    cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "# setup models    \n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(),\n",
    "                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(),\n",
    "                      LassoCV(max_iter=1e7, alphas=alphas2,\n",
    "                              random_state=42, cv=kfolds))\n",
    "\n",
    "elasticnet = make_pipeline(RobustScaler(),\n",
    "                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n",
    "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
    "                                        \n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "                                   \n",
    "\n",
    "# lightgbm = LGBMRegressor(objective='regression', \n",
    "#                                        num_leaves=4,\n",
    "#                                        learning_rate=0.01, \n",
    "#                                        n_estimators=5000,\n",
    "#                                        max_bin=200, \n",
    "#                                        bagging_fraction=0.75,\n",
    "#                                        bagging_freq=5, \n",
    "#                                        bagging_seed=7,\n",
    "#                                        feature_fraction=0.2,\n",
    "#                                        feature_fraction_seed=7,\n",
    "#                                        verbose=-1,\n",
    "#                                        #min_data_in_leaf=2,\n",
    "#                                        #min_sum_hessian_in_leaf=11\n",
    "#                                        )\n",
    "                                       \n",
    "\n",
    "# xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n",
    "#                                      max_depth=3, min_child_weight=0,\n",
    "#                                      gamma=0, subsample=0.7,\n",
    "#                                      colsample_bytree=0.7,\n",
    "#                                      objective='reg:linear', nthread=-1,\n",
    "#                                      scale_pos_weight=1, seed=27,\n",
    "#                                      reg_alpha=0.00006)\n",
    "\n",
    "# stack\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,svr,gbr),\n",
    "                                meta_regressor=lasso,\n",
    "                                use_features_in_secondary=True)\n",
    "                                \n",
    "\n",
    "print('TEST score on CV')\n",
    "\n",
    "score = cv_rmse(ridge)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(elasticnet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(svr)\n",
    "print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "# score = cv_rmse(lightgbm)\n",
    "# print(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "# score = cv_rmse(xgboost)\n",
    "# print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Fit\n",
      "2019-11-18 14:39:03.865949 StackingCVRegressor\n",
      "2019-11-18 14:40:54.255329 elasticnet\n",
      "2019-11-18 14:40:58.726705 lasso\n",
      "2019-11-18 14:40:59.877704 ridge\n",
      "2019-11-18 14:41:01.870704 svr\n",
      "2019-11-18 14:41:03.016187 GradientBoosting\n",
      "RMSLE score on train data:\n",
      "0.08024439187766624\n",
      "Predict submission 2019-11-18 14:41:14.526352\n",
      "Save submission 2019-11-18 14:41:16.154400\n"
     ]
    }
   ],
   "source": [
    "print('START Fit')\n",
    "print(datetime.now(), 'StackingCVRegressor')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "print(datetime.now(), 'elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "print(datetime.now(), 'lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "print(datetime.now(), 'ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print(datetime.now(), 'svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print(datetime.now(), 'GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "# print(datetime.now(), 'xgboost')\n",
    "# xgb_model_full_data = xgboost.fit(X, y)\n",
    "# print(datetime.now(), 'lightgbm')\n",
    "# lgb_model_full_data = lightgbm.fit(X, y)\n",
    "\n",
    "\n",
    "def blend_models_predict(X):\n",
    "    return ((0.2 * elastic_model_full_data.predict(X)) + \\\n",
    "            (0.2 * lasso_model_full_data.predict(X)) + \\\n",
    "            (0.1 * ridge_model_full_data.predict(X)) + \\\n",
    "            (0.15 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.1 * gbr_model_full_data.predict(X)) + \\\n",
    "#             (0.15 * xgb_model_full_data.predict(X)) + \\\n",
    "#             (0.1 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.25 * stack_gen_model.predict(np.array(X))))\n",
    "            \n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y, blend_models_predict(X)))\n",
    "\n",
    "print('Predict submission', datetime.now(),)\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))\n",
    "\n",
    "# this kernel gave a score 0.114\n",
    "# let's up it by mixing with the top kernels\n",
    "\n",
    "# print('Blend with Top Kernals submissions', datetime.now(),)\n",
    "# sub_1 = pd.read_csv('../input/top-10-0-10943-stacking-mice-and-brutal-force/House_Prices_submit.csv')\n",
    "# sub_2 = pd.read_csv('../input/hybrid-svm-benchmark-approach-0-11180-lb-top-2/hybrid_solution.csv')\n",
    "# sub_3 = pd.read_csv('../input/lasso-model-for-regression-problem/lasso_sol22_Median.csv')\n",
    "\n",
    "# submission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n",
    "#                                 (0.25 * sub_1.iloc[:,1]) + \n",
    "#                                 (0.25 * sub_2.iloc[:,1]) + \n",
    "#                                 (0.25 * sub_3.iloc[:,1]))\n",
    "# #\n",
    "                                \n",
    "# # Brutal approach to deal with predictions close to outer range \n",
    "# q1 = submission['SalePrice'].quantile(0.0042)\n",
    "# q2 = submission['SalePrice'].quantile(0.99)\n",
    "\n",
    "# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n",
    "# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n",
    "\n",
    "submission.to_csv(\"House_price_submission_v1.csv\", index=False)\n",
    "print('Save submission', datetime.now(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
